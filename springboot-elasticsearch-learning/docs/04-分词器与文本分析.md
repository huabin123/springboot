# 04-分词器与文本分析

## 引言

分词是全文搜索的基础。理解分词器的工作原理，是掌握Elasticsearch的关键一步。

本文将深入探讨：**为什么需要分词？分词器是如何工作的？中文分词有什么特殊性？**

---

## 问题1：为什么需要分词？

### 1.1 分词的必要性

#### 场景1：英文搜索

```
文档："I love Elasticsearch"
用户搜索："love"

不分词的情况：
- 存储："I love Elasticsearch"（整个字符串）
- 搜索："love"
- 匹配：失败 ❌（需要精确匹配整个字符串）

分词后的情况：
- 存储：["I", "love", "Elasticsearch"]（分词后的词列表）
- 搜索："love"
- 匹配：成功 ✓（在词列表中找到"love"）
```

#### 场景2：中文搜索

```
文档："我爱Elasticsearch"
用户搜索："Elasticsearch"

不分词的情况：
- 存储："我爱Elasticsearch"
- 搜索："Elasticsearch"
- 匹配：失败 ❌（子串匹配，但无法建立倒排索引）

分词后的情况：
- 存储：["我", "爱", "Elasticsearch"]
- 搜索："Elasticsearch"
- 匹配：成功 ✓
```

### 1.2 分词的作用

```
作用1：建立倒排索引
- 将文本拆分成词
- 为每个词建立倒排索引
- 实现快速搜索

作用2：支持全文搜索
- 用户可以搜索文档中的任意词
- 不需要精确匹配整个文档

作用3：提升搜索体验
- 支持部分匹配
- 支持同义词
- 支持词干提取
```

### 1.3 不分词会有什么问题？

```
问题1：无法全文搜索
- 只能精确匹配整个字段
- 用户体验差

问题2：无法建立有效的倒排索引
- 倒排索引的粒度太粗
- 搜索效率低

问题3：无法支持高级功能
- 无法同义词
- 无法词干提取
- 无法停用词过滤
```

---

## 问题2：分词器的工作原理是什么？

### 2.1 分词器的组成

分词器（Analyzer）由三部分组成：

```
┌─────────────────────────────────────────────────────┐
│                    Analyzer                          │
│                    (分词器)                          │
├─────────────────────────────────────────────────────┤
│                                                      │
│  1. Character Filter (字符过滤器)                    │
│     ↓                                                │
│     作用：预处理文本，过滤特殊字符                    │
│     示例：HTML标签过滤、特殊字符映射                  │
│                                                      │
│  2. Tokenizer (分词器)                               │
│     ↓                                                │
│     作用：将文本拆分成词（Token）                     │
│     示例：按空格分词、按标点分词                      │
│                                                      │
│  3. Token Filter (词元过滤器)                        │
│     ↓                                                │
│     作用：处理词，如转小写、去停用词、同义词          │
│     示例：lowercase、stop、synonym                   │
│                                                      │
│  输出：词列表（Token List）                          │
└─────────────────────────────────────────────────────┘
```

### 2.2 分词处理流程

```
完整的分词流程：

原始文本
  ↓
┌─────────────────────┐
│ Character Filter 1  │  HTML标签过滤
│ <p>Hello</p>        │  → Hello
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ Character Filter 2  │  特殊字符映射
│ Hello & World       │  → Hello and World
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ Tokenizer           │  按空格分词
│ Hello and World     │  → [Hello, and, World]
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ Token Filter 1      │  转小写
│ [Hello, and, World] │  → [hello, and, world]
└──────────┬──────────┘
           ↓
┌─────────────────────┐
│ Token Filter 2      │  去停用词
│ [hello, and, world] │  → [hello, world]
└──────────┬──────────┘
           ↓
最终词列表：[hello, world]
```

### 2.3 Character Filter（字符过滤器）

#### 作用

```
作用：
- 在分词前预处理文本
- 过滤或替换特殊字符
- 可以有多个，按顺序执行
```

#### 常用的Character Filter

```
1. HTML Strip Character Filter
   作用：去除HTML标签
   示例：
   输入："<p>Hello <b>World</b></p>"
   输出："Hello World"

2. Mapping Character Filter
   作用：字符映射替换
   示例：
   配置："& => and"
   输入："Tom & Jerry"
   输出："Tom and Jerry"

3. Pattern Replace Character Filter
   作用：正则表达式替换
   示例：
   配置：将数字替换为#
   输入："iPhone 14 Pro"
   输出："iPhone ## Pro"
```

#### 配置示例

```json
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter": {
          "type": "mapping",
          "mappings": [
            "& => and",
            "| => or"
          ]
        }
      }
    }
  }
}
```

### 2.4 Tokenizer（分词器）

#### 作用

```
作用：
- 将文本拆分成词（Token）
- 每个Analyzer只能有一个Tokenizer
- 是分词的核心组件
```

#### 常用的Tokenizer

```
1. Standard Tokenizer（标准分词器）
   规则：按空格和标点符号分词
   示例：
   输入："Hello, World! 123"
   输出：["Hello", "World", "123"]

2. Whitespace Tokenizer（空格分词器）
   规则：只按空格分词
   示例：
   输入："Hello, World!"
   输出：["Hello,", "World!"]

3. Keyword Tokenizer（关键词分词器）
   规则：不分词，整个文本作为一个词
   示例：
   输入："Hello World"
   输出：["Hello World"]

4. Pattern Tokenizer（正则分词器）
   规则：按正则表达式分词
   示例：
   配置：按逗号分词
   输入："apple,banana,orange"
   输出：["apple", "banana", "orange"]

5. UAX URL Email Tokenizer
   规则：识别URL和Email，不拆分
   示例：
   输入："Visit https://elastic.co or email@example.com"
   输出：["Visit", "https://elastic.co", "or", "email@example.com"]
```

### 2.5 Token Filter（词元过滤器）

#### 作用

```
作用：
- 处理Tokenizer产生的词
- 可以有多个，按顺序执行
- 支持添加、删除、修改词
```

#### 常用的Token Filter

```
1. Lowercase Token Filter（小写过滤器）
   作用：将词转为小写
   示例：
   输入：["Hello", "World"]
   输出：["hello", "world"]

2. Stop Token Filter（停用词过滤器）
   作用：去除停用词（如a、an、the、is等）
   示例：
   输入：["this", "is", "a", "test"]
   输出：["test"]

3. Synonym Token Filter（同义词过滤器）
   作用：添加同义词
   示例：
   配置："quick => fast"
   输入：["quick", "brown"]
   输出：["quick", "fast", "brown"]

4. Stemmer Token Filter（词干提取过滤器）
   作用：提取词干（如running → run）
   示例：
   输入：["running", "runs", "ran"]
   输出：["run", "run", "run"]

5. Edge NGram Token Filter（边缘N-gram过滤器）
   作用：生成前缀词（用于自动补全）
   示例：
   配置：min=1, max=3
   输入：["hello"]
   输出：["h", "he", "hel", "hello"]
```

### 2.6 完整示例

```json
// 定义自定义分词器
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter": {
          "type": "mapping",
          "mappings": ["& => and"]
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard"
        }
      },
      "filter": {
        "my_stop_filter": {
          "type": "stop",
          "stopwords": ["is", "a", "the"]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "char_filter": ["my_char_filter"],
          "tokenizer": "my_tokenizer",
          "filter": ["lowercase", "my_stop_filter"]
        }
      }
    }
  }
}

// 测试分词效果
POST /_analyze
{
  "analyzer": "my_analyzer",
  "text": "This is a Test & Example"
}

// 结果
{
  "tokens": [
    {"token": "this"},
    {"token": "test"},
    {"token": "and"},
    {"token": "example"}
  ]
}
```

---

## 问题3：ES内置了哪些分词器？

### 3.1 Standard Analyzer（标准分词器）

```
特点：
- ES的默认分词器
- 适用于大多数语言
- 按空格和标点符号分词

组成：
- Character Filter：无
- Tokenizer：Standard Tokenizer
- Token Filter：
  - Lowercase Token Filter
  - Stop Token Filter（可选）

示例：
输入："The Quick Brown Fox Jumps!"
输出：["the", "quick", "brown", "fox", "jumps"]

适用场景：
- 英文文本
- 通用场景
```

### 3.2 Simple Analyzer（简单分词器）

```
特点：
- 按非字母字符分词
- 转为小写

组成：
- Tokenizer：Lowercase Tokenizer

示例：
输入："The Quick-Brown Fox123"
输出：["the", "quick", "brown", "fox"]

适用场景：
- 简单的英文文本
- 不需要复杂处理
```

### 3.3 Whitespace Analyzer（空格分词器）

```
特点：
- 只按空格分词
- 不转小写
- 保留标点符号

组成：
- Tokenizer：Whitespace Tokenizer

示例：
输入："The Quick-Brown Fox!"
输出：["The", "Quick-Brown", "Fox!"]

适用场景：
- 需要保留大小写
- 需要保留标点符号
```

### 3.4 Stop Analyzer（停用词分词器）

```
特点：
- 去除停用词
- 转为小写

组成：
- Tokenizer：Lowercase Tokenizer
- Token Filter：Stop Token Filter

示例：
输入："The quick brown fox"
输出：["quick", "brown", "fox"]
（去除了"the"）

适用场景：
- 需要去除停用词
- 减少索引大小
```

### 3.5 Keyword Analyzer（关键词分词器）

```
特点：
- 不分词
- 整个文本作为一个词

组成：
- Tokenizer：Keyword Tokenizer

示例：
输入："The Quick Brown Fox"
输出：["The Quick Brown Fox"]

适用场景：
- 精确匹配
- 邮箱、ID等
```

### 3.6 Pattern Analyzer（正则分词器）

```
特点：
- 按正则表达式分词
- 灵活性高

组成：
- Tokenizer：Pattern Tokenizer
- Token Filter：Lowercase、Stop

示例：
配置：按逗号分词
输入："apple,banana,orange"
输出：["apple", "banana", "orange"]

适用场景：
- 特殊格式的文本
- 自定义分词规则
```

### 3.7 Language Analyzer（语言分词器）

```
特点：
- 针对特定语言优化
- 支持30+种语言
- 包含词干提取

常用的语言分词器：
- english：英语
- french：法语
- german：德语
- spanish：西班牙语

示例（english）：
输入："running runs ran"
输出：["run", "run", "run"]
（词干提取）

适用场景：
- 特定语言的文本
- 需要词干提取
```

### 3.8 分词器对比

```
┌──────────────┬────────┬────────┬────────┬──────────┐
│ 分词器       │ 分词   │ 小写   │ 停用词 │ 词干提取 │
├──────────────┼────────┼────────┼────────┼──────────┤
│ Standard     │ ✓      │ ✓      │ ✗      │ ✗        │
│ Simple       │ ✓      │ ✓      │ ✗      │ ✗        │
│ Whitespace   │ ✓      │ ✗      │ ✗      │ ✗        │
│ Stop         │ ✓      │ ✓      │ ✓      │ ✗        │
│ Keyword      │ ✗      │ ✗      │ ✗      │ ✗        │
│ Pattern      │ ✓      │ ✓      │ ✓      │ ✗        │
│ Language     │ ✓      │ ✓      │ ✓      │ ✓        │
└──────────────┴────────┴────────┴────────┴──────────┘
```

---

## 问题4：中文分词有什么特殊性？

### 4.1 中文分词的挑战

#### 挑战1：没有天然分隔符

```
英文：
- 有空格作为天然分隔符
- "I love you" → ["I", "love", "you"]

中文：
- 没有空格
- "我爱你" → ???

问题：
如何确定分词边界？
```

#### 挑战2：歧义性

```
示例1："结婚的和尚未结婚的"
分词方式1：["结婚", "的", "和", "尚未", "结婚", "的"]
分词方式2：["结婚", "的", "和尚", "未", "结婚", "的"]

示例2："乒乓球拍卖完了"
分词方式1：["乒乓球", "拍卖", "完", "了"]
分词方式2：["乒乓球拍", "卖", "完", "了"]

问题：
如何选择正确的分词方式？
```

#### 挑战3：新词识别

```
示例：
"我在用ChatGPT"
"今天去打卡网红店"

问题：
- "ChatGPT"是新词，词典中没有
- "打卡"是网络新词
- 如何识别和处理？
```

### 4.2 中文分词算法

#### 算法1：基于词典的分词

```
原理：
- 维护一个词典
- 按照词典匹配文本

方法：
1. 正向最大匹配（从左到右）
2. 逆向最大匹配（从右到左）
3. 双向最大匹配

示例（正向最大匹配）：
文本："我爱北京天安门"
词典：["我", "爱", "北京", "天安门", "天安", "门"]

匹配过程：
1. "我爱北京天安门" → 匹配"我" ✓
2. "爱北京天安门" → 匹配"爱" ✓
3. "北京天安门" → 匹配"北京" ✓
4. "天安门" → 匹配"天安门" ✓

结果：["我", "爱", "北京", "天安门"]

优点：
- 速度快
- 实现简单

缺点：
- 无法处理歧义
- 无法识别新词
```

#### 算法2：基于统计的分词

```
原理：
- 基于语料库统计
- 计算词的概率
- 选择概率最大的分词方式

方法：
1. HMM（隐马尔可夫模型）
2. CRF（条件随机场）
3. 深度学习（LSTM、BERT）

优点：
- 可以处理歧义
- 可以识别新词

缺点：
- 速度较慢
- 需要大量语料
```

#### 算法3：混合分词

```
原理：
- 结合词典和统计
- 先用词典快速分词
- 再用统计处理歧义和新词

优点：
- 速度快
- 准确率高

缺点：
- 实现复杂
```

### 4.3 常用的中文分词器

#### IK分词器（推荐）

```
特点：
- 开源、免费
- 支持自定义词典
- 两种分词模式

安装：
elasticsearch-plugin install \
  https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.17.0/elasticsearch-analysis-ik-7.17.0.zip

两种模式：
1. ik_max_word（最细粒度）
   - 穷尽所有可能的分词
   - 适合搜索

2. ik_smart（最粗粒度）
   - 最少的分词
   - 适合展示

示例：
文本："我爱北京天安门"

ik_max_word：
["我", "爱", "北京", "天安门", "天安", "门"]

ik_smart：
["我", "爱", "北京", "天安门"]
```

#### Jieba分词器

```
特点：
- Python生态的分词器
- 支持ES插件
- 三种分词模式

安装：
elasticsearch-plugin install \
  https://github.com/sing1ee/elasticsearch-jieba-plugin/releases/download/v7.17.0/elasticsearch-jieba-plugin-7.17.0.zip

三种模式：
1. 精确模式
2. 全模式
3. 搜索引擎模式
```

#### HanLP分词器

```
特点：
- 基于HanLP
- 支持多种分词算法
- 准确率高

安装：
elasticsearch-plugin install \
  https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v7.17.0/elasticsearch-analysis-hanlp-7.17.0.zip
```

### 4.4 IK分词器详解

#### 配置示例

```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "ik_max_word_analyzer": {
          "type": "ik_max_word"
        },
        "ik_smart_analyzer": {
          "type": "ik_smart"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_smart"
      }
    }
  }
}
```

#### 自定义词典

```
1. 创建自定义词典文件
路径：{ES_HOME}/plugins/ik/config/custom.dic

内容：
ChatGPT
人工智能
深度学习

2. 配置IK
编辑：{ES_HOME}/plugins/ik/config/IKAnalyzer.cfg.xml

<properties>
  <entry key="ext_dict">custom.dic</entry>
  <entry key="ext_stopwords">stopword.dic</entry>
</properties>

3. 重启ES
```

#### 分词效果对比

```
文本："我在用ChatGPT学习人工智能"

Standard Analyzer：
["我", "在", "用", "chatgpt", "学", "习", "人", "工", "智", "能"]
（单字分词，效果差）

ik_max_word：
["我", "在", "用", "chatgpt", "学习", "人工智能", "人工", "智能"]
（细粒度，适合搜索）

ik_smart：
["我", "在", "用", "chatgpt", "学习", "人工智能"]
（粗粒度，适合展示）
```

---

## 问题5：如何自定义分词器？

### 5.1 自定义分词器的场景

```
场景1：特殊业务需求
- 电商：需要识别商品型号（如iPhone 14 Pro）
- 医疗：需要识别药品名称
- 金融：需要识别股票代码

场景2：优化分词效果
- 添加自定义词典
- 添加停用词
- 添加同义词

场景3：支持拼音搜索
- 用户输入拼音
- 匹配中文
```

### 5.2 自定义分词器示例

#### 示例1：添加同义词

```json
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym_filter": {
          "type": "synonym",
          "synonyms": [
            "iPhone, 苹果手机",
            "华为, Huawei",
            "快速, 迅速, 快捷"
          ]
        }
      },
      "analyzer": {
        "my_synonym_analyzer": {
          "type": "custom",
          "tokenizer": "ik_max_word",
          "filter": ["lowercase", "my_synonym_filter"]
        }
      }
    }
  }
}

// 测试
POST /_analyze
{
  "analyzer": "my_synonym_analyzer",
  "text": "iPhone很快"
}

// 结果
{
  "tokens": [
    {"token": "iphone"},
    {"token": "苹果手机"},  // 同义词
    {"token": "很"},
    {"token": "快"},
    {"token": "迅速"},      // 同义词
    {"token": "快捷"}       // 同义词
  ]
}
```

#### 示例2：添加拼音分词

```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "pinyin_analyzer": {
          "tokenizer": "ik_max_word",
          "filter": ["pinyin_filter"]
        }
      },
      "filter": {
        "pinyin_filter": {
          "type": "pinyin",
          "keep_first_letter": true,
          "keep_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "lowercase": true
        }
      }
    }
  }
}

// 测试
POST /_analyze
{
  "analyzer": "pinyin_analyzer",
  "text": "苹果手机"
}

// 结果
{
  "tokens": [
    {"token": "苹果"},
    {"token": "pingguo"},    // 全拼
    {"token": "pg"},         // 首字母
    {"token": "手机"},
    {"token": "shouji"},
    {"token": "sj"}
  ]
}
```

#### 示例3：自定义停用词

```json
{
  "settings": {
    "analysis": {
      "filter": {
        "my_stop_filter": {
          "type": "stop",
          "stopwords": ["的", "了", "是", "在", "我", "有", "和"]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "ik_max_word",
          "filter": ["lowercase", "my_stop_filter"]
        }
      }
    }
  }
}
```

### 5.3 最佳实践

```
1. 索引时使用细粒度分词（ik_max_word）
   - 建立更多的倒排索引
   - 提高召回率

2. 搜索时使用粗粒度分词（ik_smart）
   - 减少无关结果
   - 提高精确度

3. 自定义词典
   - 添加业务相关的专业词汇
   - 定期更新

4. 合理使用同义词
   - 提高搜索覆盖率
   - 注意性能影响

5. 谨慎使用停用词
   - 可能影响搜索结果
   - 根据业务需求决定
```

---

## 总结

### 分词的核心作用

```
1. 建立倒排索引的基础
2. 支持全文搜索
3. 提升搜索体验
```

### 分词器的组成

```
1. Character Filter：字符过滤
2. Tokenizer：分词
3. Token Filter：词处理
```

### 中文分词的特殊性

```
1. 没有天然分隔符
2. 存在歧义性
3. 需要识别新词
4. 推荐使用IK分词器
```

### 自定义分词器

```
1. 添加同义词
2. 添加拼音支持
3. 自定义停用词
4. 根据业务需求定制
```

---

**下一步**：深入理解搜索与查询原理

**相关文档**：
- [05-搜索与查询原理.md](./05-搜索与查询原理.md)
- [09-索引设计与Mapping配置.md](./09-索引设计与Mapping配置.md)
