# 06-分布式架构与数据分片

## 本章概述

本章深入剖析Elasticsearch的分布式架构设计，重点解决以下问题：
- **问题1**：ES如何实现分布式存储？为什么要分片？
- **问题2**：数据写入流程是什么样的？
- **问题3**：数据读取流程是什么样的？
- **问题4**：分片机制为什么要这样设计？
- **问题5**：副本机制如何保证高可用？

---

## 问题1：ES如何实现分布式存储？为什么要分片？

### 1.1 为什么需要分布式存储？

#### 场景1：单机存储容量限制

**问题**：假设你有1TB的数据需要索引，单机硬盘只有500GB。

**传统方案的困境**：
```
单机MySQL：
- 硬盘容量：500GB < 1TB ❌
- 解决方案：换更大的硬盘（垂直扩展）
- 问题：硬件成本高，有上限
```

**ES的分布式方案**：
```
ES集群：
- 节点1：存储500GB
- 节点2：存储500GB
- 总容量：1TB ✅
- 优势：水平扩展，成本低
```

#### 场景2：单机性能瓶颈

**问题**：假设你的搜索QPS达到10000/s，单机只能支撑1000/s。

**传统方案的困境**：
```
单机处理：
- 处理能力：1000 QPS
- 实际需求：10000 QPS
- 差距：9000 QPS ❌
```

**ES的分布式方案**：
```
ES集群（10个节点）：
- 每个节点：1000 QPS
- 总处理能力：10000 QPS ✅
- 优势：并行处理，线性扩展
```

### 1.2 什么是分片（Shard）？

**分片定义**：
> 分片是Elasticsearch中数据的最小存储单元，是一个完整的Lucene索引。

**核心概念**：
```
索引（Index）
  ├── 主分片1（Primary Shard 1）
  │     ├── 副本分片1-1（Replica Shard 1-1）
  │     └── 副本分片1-2（Replica Shard 1-2）
  ├── 主分片2（Primary Shard 2）
  │     ├── 副本分片2-1（Replica Shard 2-1）
  │     └── 副本分片2-2（Replica Shard 2-2）
  └── 主分片3（Primary Shard 3）
        ├── 副本分片3-1（Replica Shard 3-1）
        └── 副本分片3-2（Replica Shard 3-2）
```

**关键点**：
1. **主分片（Primary Shard）**：存储原始数据，数量在创建索引时确定，**不可修改**
2. **副本分片（Replica Shard）**：主分片的完整拷贝，数量**可以动态调整**
3. **每个分片**：都是一个独立的Lucene索引，可以独立处理搜索请求

### 1.3 分片的作用

#### 作用1：水平扩展存储容量

**示例**：
```json
// 创建索引，设置5个主分片
PUT /products
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  }
}
```

**数据分布**：
```
假设索引有1000万条数据：
- 主分片1：200万条数据
- 主分片2：200万条数据
- 主分片3：200万条数据
- 主分片4：200万条数据
- 主分片5：200万条数据

每个分片可以存储在不同的节点上，实现水平扩展
```

#### 作用2：并行处理提升性能

**搜索请求的并行处理**：
```
用户搜索请求：GET /products/_search?q=手机

执行流程：
1. 协调节点接收请求
2. 将请求分发到5个主分片（或副本）
3. 5个分片并行搜索
4. 协调节点合并结果
5. 返回给用户

性能提升：
- 单分片搜索：100ms
- 5个分片并行：100ms（而不是500ms）
- 吞吐量提升：5倍
```

#### 作用3：故障隔离

**示例**：
```
节点1故障：
- 主分片1丢失
- 但副本分片1-1在节点2上
- 自动提升副本为主分片
- 服务不中断 ✅
```

### 1.4 分片数量如何确定？

#### 经验公式

```
主分片数量 = 预估数据量 / 单分片最大容量

建议：
- 单分片最大容量：20-50GB
- 单分片最大文档数：不超过20亿
- 单节点分片数：不超过1000个
```

#### 实际案例

**案例1：小型应用（数据量 < 100GB）**
```json
{
  "settings": {
    "number_of_shards": 3,      // 3个主分片
    "number_of_replicas": 1     // 1个副本
  }
}

说明：
- 数据量：100GB
- 单分片：33GB（100GB / 3）
- 总分片数：6个（3主 + 3副本）
- 节点数：3个（推荐）
```

**案例2：中型应用（数据量 500GB - 1TB）**
```json
{
  "settings": {
    "number_of_shards": 20,     // 20个主分片
    "number_of_replicas": 1     // 1个副本
  }
}

说明：
- 数据量：1TB
- 单分片：50GB（1TB / 20）
- 总分片数：40个（20主 + 20副本）
- 节点数：10个（推荐）
```

**案例3：大型应用（数据量 > 10TB）**
```json
{
  "settings": {
    "number_of_shards": 500,    // 500个主分片
    "number_of_replicas": 2     // 2个副本
  }
}

说明：
- 数据量：10TB
- 单分片：20GB（10TB / 500）
- 总分片数：1500个（500主 + 1000副本）
- 节点数：50个（推荐）
```

### 1.5 分片过多或过少的问题

#### 分片过少的问题

```
问题1：性能瓶颈
- 并行度低
- 单分片压力大
- 无法充分利用集群资源

问题2：扩展性差
- 无法添加更多节点来提升性能
- 单分片数据量过大，恢复时间长

示例：
索引：1TB数据，只有1个主分片
- 单分片：1TB（过大）
- 搜索性能：低
- 扩展性：差
```

#### 分片过多的问题

```
问题1：资源开销大
- 每个分片都是一个Lucene索引
- 需要占用文件句柄、内存
- 集群元数据管理开销大

问题2：搜索性能下降
- 需要合并更多分片的结果
- 协调节点压力大

示例：
索引：100GB数据，设置100个主分片
- 单分片：1GB（过小）
- 总分片数：200个（100主 + 100副本）
- 资源浪费：严重
- 搜索性能：反而下降
```

#### 最佳实践

```
✅ 推荐配置：
1. 单分片大小：20-50GB
2. 单分片文档数：< 20亿
3. 单节点分片数：< 1000个
4. 主分片数：根据数据量和节点数综合考虑

❌ 避免：
1. 单分片 > 50GB
2. 分片数 > 节点数 * 10
3. 创建索引后修改主分片数（不支持）
```

---

## 问题2：数据写入流程是什么样的？

### 2.1 写入流程概览

```
客户端写入请求
    ↓
协调节点（Coordinating Node）
    ↓
路由计算（确定主分片）
    ↓
主分片（Primary Shard）
    ↓
写入内存缓冲区（Memory Buffer）
    ↓
写入事务日志（Translog）
    ↓
同步到副本分片（Replica Shard）
    ↓
返回成功响应
    ↓
定期刷新（Refresh）→ 数据可搜索
    ↓
定期合并（Flush）→ 数据持久化
```

### 2.2 详细写入流程

#### 步骤1：客户端发送写入请求

```java
// Java客户端示例
IndexRequest request = new IndexRequest("products");
request.id("1");
request.source(
    "name", "iPhone 14",
    "price", 5999,
    "category", "手机"
);

IndexResponse response = client.index(request, RequestOptions.DEFAULT);
```

#### 步骤2：协调节点接收请求

```
协调节点的职责：
1. 接收客户端请求
2. 计算文档应该路由到哪个分片
3. 转发请求到对应的主分片
4. 等待主分片和副本分片的响应
5. 返回结果给客户端

注意：任何节点都可以作为协调节点
```

#### 步骤3：路由计算

**路由算法**：
```java
// ES的路由算法
shard_num = hash(routing_value) % num_primary_shards

参数说明：
- routing_value：默认是文档ID，也可以自定义
- num_primary_shards：主分片数量

示例：
假设索引有5个主分片，文档ID为"1"
shard_num = hash("1") % 5 = 3
→ 文档将被写入主分片3
```

**为什么主分片数不能修改？**
```
原因：路由算法依赖主分片数

如果修改主分片数：
1. 原有文档的路由结果会改变
2. 查询时找不到原有文档
3. 数据完全混乱

示例：
原来：hash("1") % 5 = 3 → 分片3
修改后：hash("1") % 10 = 1 → 分片1
→ 查询文档"1"时会去分片1查找，但实际在分片3 ❌
```

#### 步骤4：主分片写入

**写入流程**：
```
1. 写入内存缓冲区（Memory Buffer）
   - 速度快，但数据不可搜索
   - 占用JVM堆内存

2. 写入事务日志（Translog）
   - 保证数据不丢失
   - 存储在磁盘上
   - 默认每5秒或每次请求后fsync

3. 返回响应给协调节点
```

**内存缓冲区 vs Translog**：
```
内存缓冲区（Memory Buffer）：
- 作用：提升写入性能
- 位置：JVM堆内存
- 持久化：否
- 可搜索：否

事务日志（Translog）：
- 作用：保证数据不丢失
- 位置：磁盘
- 持久化：是
- 可搜索：否
```

#### 步骤5：同步到副本分片

**同步策略**：
```json
// 写入一致性配置
PUT /products/_doc/1
{
  "name": "iPhone 14"
}

// 默认行为：等待主分片写入成功即返回
// 可配置：wait_for_active_shards

// 等待所有副本写入成功
PUT /products/_doc/1?wait_for_active_shards=all
{
  "name": "iPhone 14"
}
```

**同步流程**：
```
主分片写入成功后：
1. 并行转发请求到所有副本分片
2. 副本分片执行相同的写入操作
3. 副本分片返回响应
4. 主分片等待副本响应（根据配置）
5. 返回成功给协调节点
```

#### 步骤6：Refresh - 数据可搜索

**Refresh机制**：
```
作用：将内存缓冲区的数据写入文件系统缓存，生成新的Segment

流程：
内存缓冲区（Memory Buffer）
    ↓ Refresh（默认1秒）
文件系统缓存（OS Cache）
    ↓ 生成Segment
数据可搜索 ✅

注意：
- 数据在文件系统缓存中，还未持久化到磁盘
- 如果此时断电，数据会丢失
- 但有Translog保护，可以恢复
```

**Refresh配置**：
```json
// 设置Refresh间隔
PUT /products/_settings
{
  "index.refresh_interval": "30s"  // 默认1s
}

// 禁用自动Refresh（批量导入时）
PUT /products/_settings
{
  "index.refresh_interval": "-1"
}

// 手动触发Refresh
POST /products/_refresh
```

#### 步骤7：Flush - 数据持久化

**Flush机制**：
```
作用：将文件系统缓存的数据刷新到磁盘，清空Translog

流程：
文件系统缓存（OS Cache）
    ↓ Flush（默认30分钟或Translog达到512MB）
磁盘（Disk）
    ↓
数据持久化 ✅
    ↓
清空Translog

触发条件：
1. 定期触发：默认30分钟
2. Translog过大：默认512MB
3. 手动触发：POST /products/_flush
```

### 2.3 完整写入流程图

```
┌─────────────┐
│ 客户端请求   │
└──────┬──────┘
       │
       ↓
┌─────────────────────┐
│ 协调节点              │
│ 1. 接收请求          │
│ 2. 路由计算          │
│ shard = hash(id) % 5 │
└──────┬──────────────┘
       │
       ↓
┌─────────────────────────────────┐
│ 主分片（Primary Shard）          │
│ ┌─────────────────────────────┐ │
│ │ 1. 写入Memory Buffer        │ │
│ │    - 数据在内存中            │ │
│ │    - 不可搜索                │ │
│ └─────────────────────────────┘ │
│ ┌─────────────────────────────┐ │
│ │ 2. 写入Translog             │ │
│ │    - 数据持久化到磁盘        │ │
│ │    - 保证不丢失              │ │
│ └─────────────────────────────┘ │
└──────┬──────────────────────────┘
       │
       ↓
┌─────────────────────────────────┐
│ 副本分片（Replica Shard）        │
│ - 并行同步到所有副本             │
│ - 执行相同的写入操作             │
└──────┬──────────────────────────┘
       │
       ↓
┌─────────────────────────────────┐
│ 返回成功响应                     │
└─────────────────────────────────┘
       │
       ↓（后台异步）
┌─────────────────────────────────┐
│ Refresh（默认1秒）               │
│ - Memory Buffer → OS Cache      │
│ - 生成Segment                   │
│ - 数据可搜索 ✅                  │
└──────┬──────────────────────────┘
       │
       ↓（后台异步）
┌─────────────────────────────────┐
│ Flush（默认30分钟）              │
│ - OS Cache → Disk               │
│ - 数据持久化 ✅                  │
│ - 清空Translog                  │
└─────────────────────────────────┘
```

### 2.4 写入性能优化

#### 优化1：批量写入

```java
// ❌ 不推荐：单条写入
for (int i = 0; i < 10000; i++) {
    IndexRequest request = new IndexRequest("products")
        .id(String.valueOf(i))
        .source("name", "Product " + i);
    client.index(request, RequestOptions.DEFAULT);
}
// 性能：慢，需要10000次网络请求

// ✅ 推荐：批量写入
BulkRequest bulkRequest = new BulkRequest();
for (int i = 0; i < 10000; i++) {
    bulkRequest.add(new IndexRequest("products")
        .id(String.valueOf(i))
        .source("name", "Product " + i));
}
client.bulk(bulkRequest, RequestOptions.DEFAULT);
// 性能：快，只需要1次网络请求
```

#### 优化2：调整Refresh间隔

```json
// 批量导入时，禁用自动Refresh
PUT /products/_settings
{
  "index.refresh_interval": "-1"
}

// 批量导入数据...

// 导入完成后，恢复Refresh
PUT /products/_settings
{
  "index.refresh_interval": "1s"
}

// 手动触发一次Refresh
POST /products/_refresh

性能提升：5-10倍
```

#### 优化3：增加副本数前先导入数据

```json
// ✅ 推荐流程：
// 1. 创建索引时，设置0个副本
PUT /products
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 0  // 先不创建副本
  }
}

// 2. 批量导入数据

// 3. 导入完成后，增加副本
PUT /products/_settings
{
  "number_of_replicas": 1
}

原因：
- 写入时不需要同步到副本，速度更快
- 数据导入完成后，再创建副本
- 副本数据通过复制主分片获得，比逐条同步快
```

#### 优化4：调整Translog刷新策略

```json
// 默认配置（安全）
PUT /products/_settings
{
  "index.translog.durability": "request",  // 每次请求后fsync
  "index.translog.sync_interval": "5s"     // 5秒fsync一次
}

// 高性能配置（可能丢失5秒数据）
PUT /products/_settings
{
  "index.translog.durability": "async",    // 异步fsync
  "index.translog.sync_interval": "5s"     // 5秒fsync一次
}

性能提升：2-3倍
风险：断电可能丢失5秒内的数据
```

---

## 问题3：数据读取流程是什么样的？

### 3.1 读取流程概览

```
客户端查询请求
    ↓
协调节点（Coordinating Node）
    ↓
Query Phase（查询阶段）
    ↓
分发请求到所有分片（主分片或副本）
    ↓
各分片并行查询，返回文档ID和评分
    ↓
协调节点合并排序，确定Top N
    ↓
Fetch Phase（获取阶段）
    ↓
获取Top N的完整文档
    ↓
返回给客户端
```

### 3.2 详细读取流程

#### 步骤1：客户端发送查询请求

```java
// Java客户端示例
SearchRequest searchRequest = new SearchRequest("products");
SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
sourceBuilder.query(QueryBuilders.matchQuery("name", "手机"));
sourceBuilder.from(0);
sourceBuilder.size(10);
searchRequest.source(sourceBuilder);

SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);
```

#### 步骤2：协调节点接收请求

```
协调节点的职责：
1. 接收客户端查询请求
2. 将请求分发到所有相关分片
3. 合并各分片的查询结果
4. 获取完整文档
5. 返回给客户端

分片选择策略：
- 每个主分片或其任意一个副本
- 轮询（Round-robin）
- 优先选择本地分片
- 优先选择负载低的分片
```

#### 步骤3：Query Phase（查询阶段）

**流程**：
```
协调节点
    ↓
并行发送查询请求到所有分片
    ↓
┌──────────┬──────────┬──────────┐
│ 分片1     │ 分片2     │ 分片3     │
│ 查询      │ 查询      │ 查询      │
│ 排序      │ 排序      │ 排序      │
│ 返回Top10 │ 返回Top10 │ 返回Top10 │
└──────────┴──────────┴──────────┘
    ↓
协调节点收集结果（30个文档ID和评分）
    ↓
全局排序，确定最终Top10
```

**返回数据**：
```json
// 每个分片返回的数据（轻量级）
[
  {
    "_id": "1",
    "_score": 8.5,
    "_shard": "[products][0]"
  },
  {
    "_id": "5",
    "_score": 7.2,
    "_shard": "[products][0]"
  },
  // ... Top 10
]

注意：
- 只返回文档ID和评分
- 不返回完整文档内容
- 减少网络传输量
```

#### 步骤4：Fetch Phase（获取阶段）

**流程**：
```
协调节点确定最终Top10的文档ID
    ↓
根据文档ID，向对应分片请求完整文档
    ↓
┌──────────┬──────────┬──────────┐
│ 分片1     │ 分片2     │ 分片3     │
│ 获取文档1 │ 获取文档5 │ 获取文档9 │
│ 获取文档2 │ 获取文档6 │          │
└──────────┴──────────┴──────────┘
    ↓
协调节点收集完整文档
    ↓
应用高亮、聚合等后处理
    ↓
返回给客户端
```

**返回数据**：
```json
// 完整文档
{
  "hits": {
    "total": 1000,
    "max_score": 8.5,
    "hits": [
      {
        "_id": "1",
        "_score": 8.5,
        "_source": {
          "name": "iPhone 14 Pro",
          "price": 7999,
          "category": "手机"
        }
      },
      // ... Top 10完整文档
    ]
  }
}
```

### 3.3 为什么要分两个阶段？

#### 原因1：减少网络传输

```
假设查询Top10，索引有5个分片：

❌ 如果不分阶段（直接返回完整文档）：
- 每个分片返回Top10完整文档
- 协调节点收到50个完整文档
- 网络传输：50个文档的数据量
- 最终只需要10个文档，浪费了40个文档的传输

✅ 分两个阶段：
- Query Phase：每个分片返回10个文档ID和评分（轻量级）
- 协调节点确定最终Top10
- Fetch Phase：只获取10个完整文档
- 网络传输：10个文档的数据量
- 节省：80%的网络传输
```

#### 原因2：减少内存占用

```
协调节点内存占用：

❌ 不分阶段：
- 需要在内存中存储50个完整文档
- 内存占用：高

✅ 分两个阶段：
- Query Phase：只存储50个文档ID和评分
- Fetch Phase：只存储10个完整文档
- 内存占用：低
```

### 3.4 完整读取流程图

```
┌─────────────┐
│ 客户端请求   │
│ GET /products/_search?q=手机&size=10
└──────┬──────┘
       │
       ↓
┌─────────────────────────────────┐
│ 协调节点                         │
│ 1. 解析查询请求                  │
│ 2. 确定需要查询的分片            │
└──────┬──────────────────────────┘
       │
       ↓
┌─────────────────────────────────┐
│ Query Phase（查询阶段）          │
│                                 │
│ ┌─────────┐ ┌─────────┐ ┌─────────┐
│ │ 分片1    │ │ 分片2    │ │ 分片3    │
│ │ 查询     │ │ 查询     │ │ 查询     │
│ │ 排序     │ │ 排序     │ │ 排序     │
│ │ Top10   │ │ Top10   │ │ Top10   │
│ └────┬────┘ └────┬────┘ └────┬────┘
│      │           │           │
│      └───────────┼───────────┘
│                  ↓
│ 协调节点收集30个文档ID和评分
│ 全局排序，确定最终Top10
└──────┬──────────────────────────┘
       │
       ↓
┌─────────────────────────────────┐
│ Fetch Phase（获取阶段）          │
│                                 │
│ 根据文档ID，向对应分片请求完整文档
│                                 │
│ ┌─────────┐ ┌─────────┐ ┌─────────┐
│ │ 分片1    │ │ 分片2    │ │ 分片3    │
│ │ 文档1,2  │ │ 文档5,6  │ │ 文档9    │
│ └────┬────┘ └────┬────┘ └────┬────┘
│      │           │           │
│      └───────────┼───────────┘
│                  ↓
│ 协调节点收集10个完整文档
│ 应用高亮、聚合等后处理
└──────┬──────────────────────────┘
       │
       ↓
┌─────────────┐
│ 返回给客户端 │
└─────────────┘
```

### 3.5 深度分页问题

#### 问题描述

```java
// 查询第1000页，每页10条
SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
sourceBuilder.from(10000);  // 跳过前10000条
sourceBuilder.size(10);     // 获取10条

问题：
- 每个分片需要返回Top 10010个文档ID
- 协调节点需要处理 10010 * 分片数 个文档
- 内存占用和性能都很差
```

#### 为什么深度分页性能差？

```
假设查询第1000页（from=10000, size=10），索引有5个分片：

Query Phase：
- 每个分片返回Top 10010个文档ID和评分
- 协调节点收到 10010 * 5 = 50050 个文档ID
- 协调节点需要排序50050个文档
- 确定最终Top 10010，然后跳过前10000个

Fetch Phase：
- 获取10个完整文档

性能问题：
1. 网络传输：50050个文档ID（虽然轻量，但数量大）
2. 内存占用：协调节点需要存储50050个文档ID
3. 排序开销：对50050个文档排序

随着页数增加，性能线性下降
```

#### 解决方案

**方案1：使用Scroll API（已废弃，不推荐）**
```java
// 不推荐，ES 7.x后建议使用Search After
```

**方案2：使用Search After（推荐）**
```java
// 第一次查询
SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
sourceBuilder.query(QueryBuilders.matchAllQuery());
sourceBuilder.size(10);
sourceBuilder.sort("_id", SortOrder.ASC);  // 必须指定排序字段

SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);
SearchHit[] hits = response.getHits().getHits();

// 获取最后一条文档的排序值
Object[] sortValues = hits[hits.length - 1].getSortValues();

// 第二次查询（下一页）
sourceBuilder.searchAfter(sortValues);  // 从上次的位置继续
SearchResponse response2 = client.search(searchRequest, RequestOptions.DEFAULT);

优势：
- 不需要跳过前面的文档
- 性能稳定，不随页数增加而下降
- 适合实时滚动场景
```

**方案3：使用Point in Time + Search After（ES 7.10+推荐）**
```java
// 1. 创建Point in Time
OpenPointInTimeRequest pitRequest = new OpenPointInTimeRequest("products")
    .keepAlive(TimeValue.timeValueMinutes(1));
OpenPointInTimeResponse pitResponse = client.openPointInTime(pitRequest, RequestOptions.DEFAULT);
String pit = pitResponse.getPointInTimeId();

// 2. 使用PIT进行查询
SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
sourceBuilder.size(10);
sourceBuilder.sort("_shard_doc", SortOrder.ASC);
sourceBuilder.pointInTimeBuilder(new PointInTimeBuilder(pit));

SearchResponse response = client.search(new SearchRequest().source(sourceBuilder), RequestOptions.DEFAULT);

// 3. 下一页
Object[] sortValues = response.getHits().getHits()[9].getSortValues();
sourceBuilder.searchAfter(sortValues);

// 4. 关闭PIT
ClosePointInTimeRequest closePitRequest = new ClosePointInTimeRequest(pit);
client.closePointInTime(closePitRequest, RequestOptions.DEFAULT);

优势：
- 保证查询的一致性（数据快照）
- 性能更好
- 推荐用于大数据量导出
```

---

## 问题4：分片机制为什么要这样设计？

### 4.1 设计目标

```
1. 水平扩展：支持PB级数据存储
2. 高性能：并行处理，提升吞吐量
3. 高可用：故障自动恢复
4. 负载均衡：数据均匀分布
```

### 4.2 设计权衡

#### 权衡1：主分片数不可修改 vs 灵活性

**为什么不可修改？**
```
原因：路由算法依赖主分片数
shard_num = hash(routing_value) % num_primary_shards

如果可以修改：
1. 需要重新计算所有文档的路由
2. 需要迁移所有数据
3. 成本极高

ES的选择：
- 主分片数创建时确定，不可修改
- 副本数可以动态调整
- 需要提前规划好主分片数
```

**如何应对数据增长？**
```
方案1：创建时预留足够的主分片
- 优点：简单
- 缺点：可能浪费资源

方案2：使用Rollover API（推荐）
- 按时间或大小自动创建新索引
- 例如：logs-2024-01、logs-2024-02
- 使用索引别名统一访问

方案3：使用Shrink API缩减分片
- 将多个分片合并为更少的分片
- 适用于历史数据

方案4：使用Split API拆分分片（ES 6.1+）
- 将一个分片拆分为多个分片
- 新分片数必须是原分片数的倍数
```

#### 权衡2：分片粒度 vs 资源开销

**细粒度分片**：
```
优点：
- 并行度高
- 负载均衡好
- 扩展性强

缺点：
- 资源开销大（每个分片都是一个Lucene索引）
- 搜索时需要合并更多分片的结果
- 集群元数据管理开销大
```

**粗粒度分片**：
```
优点：
- 资源开销小
- 搜索合并开销小

缺点：
- 并行度低
- 负载均衡差
- 扩展性差
```

**ES的选择**：
```
推荐：单分片20-50GB
- 平衡性能和资源开销
- 根据实际情况调整
```

#### 权衡3：副本数 vs 成本

**多副本**：
```
优点：
- 高可用性
- 读性能提升（可以从副本读取）
- 故障恢复快

缺点：
- 存储成本高（副本数 * 数据量）
- 写入性能下降（需要同步到副本）
```

**少副本**：
```
优点：
- 存储成本低
- 写入性能高

缺点：
- 可用性低
- 读性能一般
```

**ES的选择**：
```
默认：1个副本
- 平衡可用性和成本
- 可以根据需求动态调整

生产环境推荐：
- 重要数据：2个副本
- 一般数据：1个副本
- 临时数据：0个副本
```

### 4.3 与其他分布式系统的对比

#### ES vs MongoDB

```
ES分片：
- 主分片数：创建时确定，不可修改
- 路由：hash(id) % num_shards
- 优点：路由简单，性能高
- 缺点：不够灵活

MongoDB分片：
- 分片数：可以动态增加
- 路由：基于Shard Key的范围或哈希
- 优点：灵活
- 缺点：需要配置服务器（Config Server）管理元数据
```

#### ES vs HBase

```
ES分片：
- 分片：逻辑概念，对应Lucene索引
- 副本：完整拷贝
- 故障恢复：自动提升副本为主分片

HBase分片：
- Region：物理概念，对应HFile
- 副本：通过HDFS的副本机制
- 故障恢复：依赖HDFS
```

---

## 问题5：副本机制如何保证高可用？

### 5.1 副本的作用

```
作用1：高可用
- 主分片故障时，自动提升副本为主分片
- 服务不中断

作用2：提升读性能
- 搜索请求可以路由到副本
- 分担主分片的压力

作用3：数据备份
- 防止数据丢失
```

### 5.2 副本分配策略

#### 策略1：主副分片不在同一节点

```
规则：主分片和副本分片不能在同一个节点上

示例：
节点1：主分片1、副本分片2
节点2：主分片2、副本分片1

原因：
- 如果在同一节点，节点故障会导致主副分片同时丢失
- 失去高可用的意义
```

#### 策略2：均匀分布

```
ES会尽量将分片均匀分布到所有节点上

示例（3节点，3主分片，1副本）：
节点1：主1、副2
节点2：主2、副3
节点3：主3、副1

每个节点：2个分片
负载均衡 ✅
```

#### 策略3：感知机架/可用区

```json
// 配置机架感知
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "rack_id"
  }
}

// 节点配置
node.attr.rack_id: rack1

效果：
- 主分片和副本分片不在同一机架
- 提升可用性（机架故障不影响服务）
```

### 5.3 故障恢复流程

#### 场景1：节点故障

```
初始状态（3节点）：
节点1：主1、副2
节点2：主2、副3
节点3：主3、副1

节点1故障：
1. Master节点检测到节点1不可用
2. 将节点1上的主1标记为unassigned
3. 提升节点3上的副1为主1
4. 在节点2上创建新的副1

最终状态（2节点）：
节点2：主2、副3、副1
节点3：主3、主1（原副1提升）

服务状态：
- 集群状态：Yellow（有副本未分配）
- 服务可用：是 ✅
- 数据完整：是 ✅
```

#### 场景2：节点恢复

```
节点1恢复：
1. 节点1重新加入集群
2. Master节点检测到节点1可用
3. 将部分分片迁移回节点1
4. 重新平衡分片分布

最终状态（3节点）：
节点1：主1、副2
节点2：主2、副3
节点3：主3、副1

集群状态：Green ✅
```

### 5.4 副本一致性

#### 写入一致性

```
默认行为：
- 主分片写入成功即返回
- 异步同步到副本

可配置：
PUT /products/_doc/1?wait_for_active_shards=all
{
  "name": "iPhone 14"
}

wait_for_active_shards选项：
- 1：只等待主分片（默认）
- all：等待所有副本
- 数字：等待指定数量的分片
```

#### 读取一致性

```
问题：主分片和副本分片的数据可能不一致

ES的解决方案：
1. 默认从主分片或副本随机读取
2. 使用preference参数控制读取策略

// 优先从主分片读取
GET /products/_search?preference=_primary

// 优先从本地分片读取
GET /products/_search?preference=_local

// 使用自定义路由（保证相同请求路由到相同分片）
GET /products/_search?preference=user123
```

### 5.5 副本数量建议

```
场景1：重要业务数据
- 副本数：2
- 容忍：2个节点同时故障
- 成本：3倍存储

场景2：一般业务数据
- 副本数：1
- 容忍：1个节点故障
- 成本：2倍存储

场景3：临时数据/日志
- 副本数：0
- 容忍：不容忍故障
- 成本：1倍存储

场景4：超高可用需求
- 副本数：3+
- 容忍：3+个节点同时故障
- 成本：4+倍存储
```

---

## 本章总结

### 核心要点

1. **分片机制**
   - 主分片：数据分片，创建时确定，不可修改
   - 副本分片：数据备份，可动态调整
   - 路由算法：`shard = hash(id) % num_primary_shards`

2. **写入流程**
   - 路由 → 主分片 → Memory Buffer → Translog → 副本同步
   - Refresh（1秒）：数据可搜索
   - Flush（30分钟）：数据持久化

3. **读取流程**
   - Query Phase：各分片并行查询，返回文档ID和评分
   - Fetch Phase：获取完整文档
   - 两阶段设计：减少网络传输和内存占用

4. **高可用**
   - 副本机制：主分片故障自动提升副本
   - 分片分配：主副分片不在同一节点
   - 故障恢复：自动重新分配分片

### 最佳实践

```
✅ 分片规划：
1. 单分片20-50GB
2. 提前规划主分片数
3. 使用Rollover管理时序数据

✅ 写入优化：
1. 批量写入（Bulk API）
2. 调整Refresh间隔
3. 先导入数据，再增加副本

✅ 读取优化：
1. 避免深度分页
2. 使用Search After
3. 使用Filter缓存

✅ 高可用：
1. 至少3个节点
2. 至少1个副本
3. 配置机架感知
```

### 常见问题

```
Q1：主分片数设置多少合适？
A1：预估数据量 / 50GB，建议3-20个

Q2：为什么主分片数不能修改？
A2：路由算法依赖主分片数，修改会导致数据混乱

Q3：副本数设置多少合适？
A3：一般业务1个，重要业务2个

Q4：深度分页如何优化？
A4：使用Search After或Point in Time

Q5：写入性能如何优化？
A5：批量写入、调整Refresh间隔、减少副本数
```

---

## 下一章预告

**07-集群管理与高可用**

下一章将深入讲解：
- ES集群的节点类型和职责
- Master选举机制
- 脑裂问题及解决方案
- 集群健康状态监控
- 分片分配策略
- 集群扩容和缩容

敬请期待！
